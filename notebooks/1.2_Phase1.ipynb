{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a968f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 1: Setup & Imports ---\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import joblib\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Setup GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdbf33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 2: Random Seed ---\n",
    "import random\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    print(f\"Random Seed locked to: {seed}\")\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab3b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 3: Universal Data Loader & Feature Engineering ---\n",
    "# 1. Load Sentiment Bank\n",
    "pkl_path = \"../model_artifacts/universal_sentiment_history.pkl\"\n",
    "print(f\"Loading Sentiment History from: {pkl_path}...\")\n",
    "universal_sentiment = pd.read_pickle(pkl_path)\n",
    "\n",
    "# 2. Select Top 200 Stocks (\"The Navy SEALs\")\n",
    "# We only want stocks with enough news history to be good teachers\n",
    "stock_counts = universal_sentiment['stock'].value_counts()\n",
    "valid_stocks = stock_counts[stock_counts >= 500] # Must have 500+ news items\n",
    "TARGET_TICKERS = valid_stocks.head(200).index.tolist()\n",
    "\n",
    "print(f\"âœ… Selected Top {len(TARGET_TICKERS)} Stocks for Training.\")\n",
    "print(f\"   Examples: {TARGET_TICKERS[:10]}...\")\n",
    "\n",
    "# 3. The Mega-Loader Function\n",
    "def create_universal_dataset(tickers):\n",
    "    all_data_frames = []\n",
    "    print(f\"\\nðŸ“¥ Starting Bulk Download (2010-2026)...\")\n",
    "    \n",
    "    for i, ticker in enumerate(tickers):\n",
    "        try:\n",
    "            # A. Download Price (Extended Range for Deep Learning)\n",
    "            raw_df = yf.download(ticker, start='2010-01-01', end='2026-01-01', progress=False)\n",
    "            \n",
    "            if len(raw_df) < 500: continue # Skip data-poor stocks\n",
    "            \n",
    "            # Fix MultiIndex (yfinance bug fix)\n",
    "            if isinstance(raw_df.columns, pd.MultiIndex):\n",
    "                raw_df.columns = raw_df.columns.get_level_values(0)\n",
    "            \n",
    "            # B. Merge Sentiment\n",
    "            raw_df['date_str'] = raw_df.index.strftime('%Y-%m-%d')\n",
    "            \n",
    "            # Filter sentiment DB for just this stock\n",
    "            sent_data = universal_sentiment[universal_sentiment['stock'] == ticker].copy()\n",
    "            sent_data['date_str'] = pd.to_datetime(sent_data['date']).dt.strftime('%Y-%m-%d')\n",
    "            \n",
    "            # Left Join (Price is King, Sentiment fills in gaps)\n",
    "            merged_df = pd.merge(raw_df, sent_data[['date_str', 'daily_score']], on='date_str', how='left')\n",
    "            merged_df.rename(columns={'daily_score': 'Sentiment'}, inplace=True)\n",
    "            merged_df['Sentiment'] = merged_df['Sentiment'].fillna(0.0) # No news = Neutral\n",
    "            \n",
    "            # C. Feature Engineering (The Math)\n",
    "            price_col = 'Adj Close' if 'Adj Close' in merged_df.columns else 'Close'\n",
    "            merged_df['Close_Price'] = merged_df[price_col]\n",
    "            \n",
    "            # Returns & Target\n",
    "            merged_df['Log_Ret'] = np.log(merged_df['Close_Price'] / merged_df['Close_Price'].shift(1))\n",
    "            # Target: 1 if price rises over next 5 days\n",
    "            merged_df['Target'] = (merged_df['Close_Price'].shift(-5) > merged_df['Close_Price']).astype(int)\n",
    "            \n",
    "            # Technical Indicators\n",
    "            merged_df['Vol_20'] = merged_df['Log_Ret'].rolling(20).std()\n",
    "            merged_df['Vol_200'] = merged_df['Log_Ret'].rolling(200).std()\n",
    "            merged_df['Vol_Ratio'] = merged_df['Vol_20'] / merged_df['Vol_200']\n",
    "            merged_df['Ret_1M'] = merged_df['Close_Price'].pct_change(20)\n",
    "            merged_df['Ret_3M'] = merged_df['Close_Price'].pct_change(60)\n",
    "            merged_df['Efficiency'] = merged_df['Close_Price'].diff(20).abs() / merged_df['Close_Price'].diff(1).abs().rolling(20).sum()\n",
    "            merged_df['Vol_Shock'] = merged_df['Volume'] / merged_df['Volume'].rolling(50).mean()\n",
    "            \n",
    "            # LSTM Lags (The Memory)\n",
    "            for lag in [1, 2, 3, 5, 10, 20]:\n",
    "                merged_df[f'Ret_Lag{lag}'] = merged_df['Log_Ret'].shift(lag)\n",
    "            \n",
    "            # Cleanup\n",
    "            merged_df.dropna(inplace=True)\n",
    "            all_data_frames.append(merged_df)\n",
    "            \n",
    "            if i % 20 == 0: print(f\"   âœ… Processed {i}/{len(tickers)}: {ticker}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    # D. Stack Everything\n",
    "    print(\"ðŸ”— Merging all datasets into Universal Brain...\")\n",
    "    final_df = pd.concat(all_data_frames)\n",
    "    return final_df\n",
    "\n",
    "# Execute\n",
    "feature_df = create_universal_dataset(TARGET_TICKERS)\n",
    "\n",
    "# Define Columns for Model\n",
    "RF_COLS = ['Vol_Ratio', 'Efficiency', 'Vol_Shock', 'Ret_1M', 'Ret_3M', 'Sentiment']\n",
    "LSTM_COLS = ['Log_Ret', 'Ret_Lag1', 'Ret_Lag2', 'Ret_Lag3', 'Ret_Lag5', 'Ret_Lag10', 'Sentiment']\n",
    "SEQ_LENGTH = 60\n",
    "\n",
    "print(f\"\\nðŸ”¥ FINAL TRAINING SET SIZE: {len(feature_df)} ROWS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478560cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 4: The Hybrid Model Architecture ---\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 1. LSTM (Time Series Specialist)\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, output_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :]) # Take last step\n",
    "        return out\n",
    "\n",
    "# 2. Ensemble Class (The Boss)\n",
    "class HybridEnsemble:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "        self.lstm = None \n",
    "        \n",
    "    def fit(self, X_train_rf, X_train_lstm, y_train):\n",
    "        # Train RF\n",
    "        self.rf.fit(X_train_rf, y_train)\n",
    "        \n",
    "        # Train LSTM\n",
    "        input_dim = X_train_lstm.shape[2]\n",
    "        self.lstm = LSTMModel(input_size=input_dim).to(self.device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(self.lstm.parameters(), lr=0.001)\n",
    "        \n",
    "        X_t = torch.tensor(X_train_lstm, dtype=torch.float32).to(self.device)\n",
    "        y_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        self.lstm.train()\n",
    "        for epoch in range(15): # 15 Epochs is enough for this much data\n",
    "            optimizer.zero_grad()\n",
    "            out = self.lstm(X_t)\n",
    "            loss = criterion(out, y_t)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch % 5 == 0: print(f\"   LSTM Epoch {epoch} Loss: {loss.item():.4f}\")\n",
    "            \n",
    "    def predict_proba(self, X_test_rf, X_test_lstm):\n",
    "        # RF Prediction\n",
    "        rf_prob = self.rf.predict_proba(X_test_rf)[:, 1]\n",
    "        \n",
    "        # LSTM Prediction\n",
    "        self.lstm.eval()\n",
    "        with torch.no_grad():\n",
    "            X_t = torch.tensor(X_test_lstm, dtype=torch.float32).to(self.device)\n",
    "            lstm_logits = self.lstm(X_t)\n",
    "            lstm_prob = torch.sigmoid(lstm_logits).cpu().numpy().flatten()\n",
    "            \n",
    "        # Ensemble Average\n",
    "        return (rf_prob * 0.5) + (lstm_prob * 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6def829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 5: Train the Universal Brain ---\n",
    "def create_sequences(X, y, seq_len):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        xs.append(X[i:(i+seq_len)])\n",
    "        ys.append(y[i+seq_len])\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "print(\"ðŸš€ Training Universal Model on 200 Stocks...\")\n",
    "\n",
    "# 1. Scale Data (Fit on EVERYTHING)\n",
    "scaler_rf = RobustScaler()\n",
    "X_full_rf = scaler_rf.fit_transform(feature_df[RF_COLS])\n",
    "\n",
    "scaler_lstm = RobustScaler()\n",
    "X_full_lstm = scaler_lstm.fit_transform(feature_df[LSTM_COLS])\n",
    "\n",
    "# 2. Create Sequences\n",
    "print(\"   Generating Sequences (This may take a moment)...\")\n",
    "X_seq, y_seq = create_sequences(X_full_lstm, feature_df['Target'].values, SEQ_LENGTH)\n",
    "X_rf_aligned = X_full_rf[SEQ_LENGTH:]\n",
    "\n",
    "# 3. Train\n",
    "master_model = HybridEnsemble(device)\n",
    "master_model.fit(X_rf_aligned, X_seq, y_seq)\n",
    "\n",
    "# 4. Save Artifacts\n",
    "ARTIFACTS_DIR = \"model_artifacts\"\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "\n",
    "torch.save(master_model.lstm.state_dict(), f\"{ARTIFACTS_DIR}/production_lstm.pth\")\n",
    "joblib.dump(master_model.rf, f\"{ARTIFACTS_DIR}/production_rf.pkl\")\n",
    "joblib.dump(scaler_rf, f\"{ARTIFACTS_DIR}/scaler_rf.pkl\")\n",
    "joblib.dump(scaler_lstm, f\"{ARTIFACTS_DIR}/scaler_lstm.pkl\")\n",
    "\n",
    "print(f\"âœ… Universal Brain Saved to: {ARTIFACTS_DIR}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f53b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 6: The Final Exam (Backtest on SPY) ---\n",
    "# We verify the model by testing it on the S&P 500 ETF (SPY)\n",
    "print(\"ðŸ§ª Running Validation Backtest on SPY (S&P 500)...\")\n",
    "\n",
    "# 1. Download Test Data\n",
    "test_ticker = \"SPY\"\n",
    "test_df = yf.download(test_ticker, start='2020-01-01', end='2025-01-01', progress=False) # Covid Era\n",
    "if isinstance(test_df.columns, pd.MultiIndex): test_df.columns = test_df.columns.get_level_values(0)\n",
    "\n",
    "# 2. Process (Same pipeline as training)\n",
    "# Note: In a real scenario, we would merge sentiment here too. \n",
    "# For this quick check, we assume 0 sentiment to test technicals.\n",
    "test_df['Sentiment'] = 0.0 \n",
    "test_df['Close_Price'] = test_df['Adj Close'] if 'Adj Close' in test_df else test_df['Close']\n",
    "test_df['Log_Ret'] = np.log(test_df['Close_Price'] / test_df['Close_Price'].shift(1))\n",
    "test_df['Vol_20'] = test_df['Log_Ret'].rolling(20).std()\n",
    "test_df['Vol_200'] = test_df['Log_Ret'].rolling(200).std()\n",
    "test_df['Vol_Ratio'] = test_df['Vol_20'] / test_df['Vol_200']\n",
    "test_df['Ret_1M'] = test_df['Close_Price'].pct_change(20)\n",
    "test_df['Ret_3M'] = test_df['Close_Price'].pct_change(60)\n",
    "test_df['Efficiency'] = test_df['Close_Price'].diff(20).abs() / test_df['Close_Price'].diff(1).abs().rolling(20).sum()\n",
    "test_df['Vol_Shock'] = test_df['Volume'] / test_df['Volume'].rolling(50).mean()\n",
    "for lag in [1, 2, 3, 5, 10, 20]:\n",
    "    test_df[f'Ret_Lag{lag}'] = test_df['Log_Ret'].shift(lag)\n",
    "test_df.dropna(inplace=True)\n",
    "\n",
    "# 3. Predict\n",
    "X_test_rf = scaler_rf.transform(test_df[RF_COLS])\n",
    "X_test_lstm = scaler_lstm.transform(test_df[LSTM_COLS])\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_lstm, np.zeros(len(test_df)), SEQ_LENGTH) # y dummy\n",
    "X_test_rf_aligned = X_test_rf[SEQ_LENGTH:]\n",
    "\n",
    "probs = master_model.predict_proba(X_test_rf_aligned, X_test_seq)\n",
    "\n",
    "# 4. Simulate Strategy\n",
    "# Buy if Prob > 0.55 (Confidence Threshold)\n",
    "signals = (probs > 0.55).astype(int)\n",
    "returns = test_df['Log_Ret'].iloc[SEQ_LENGTH:].values\n",
    "strategy_returns = signals * returns\n",
    "\n",
    "# 5. Plot\n",
    "cumulative_market = np.exp(np.cumsum(returns))\n",
    "cumulative_strategy = np.exp(np.cumsum(strategy_returns))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_df.index[SEQ_LENGTH:], cumulative_market, label='Buy & Hold (SPY)', color='gray')\n",
    "plt.plot(test_df.index[SEQ_LENGTH:], cumulative_strategy, label='Universal AI Strategy', color='green', linewidth=2)\n",
    "plt.title(f\"Universal Brain Performance on {test_ticker} (Unseen Data)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total Market Return: {(cumulative_market[-1]-1)*100:.2f}%\")\n",
    "print(f\"Total AI Return:     {(cumulative_strategy[-1]-1)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8ac360",
   "metadata": {},
   "source": [
    "# ðŸ“‘ Notebook 1.2 Technical Report: Phase 1 Core\n",
    "\n",
    "**Module:** 1.2 - The Hybrid Prediction Engine  \n",
    "**Core Models:** Random Forest (Regime) + LSTM (Sequence)  \n",
    "**Role:** The \"Brain\" of GenWealth (Directional Forecasting)  \n",
    "**Input:** Price History (OHLCV) + Sentiment Signals (from 1.1)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. System Objective\n",
    "The goal of this module is to train a robust, universal predictive model capable of forecasting the **5-Day Price Direction** (Up/Down) for any stock. It achieves this by fusing \"Hard Math\" (Technical Indicators) with \"Soft Signal\" (News Sentiment) into a single decision engine.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The Universal Data Pipeline\n",
    "\n",
    "### 2.1 The \"Navy SEALs\" Selection\n",
    "We do not train on random penny stocks. We filter for high-quality data:\n",
    "* **Criterion:** Stocks with >500 individual news items in our sentiment database.\n",
    "* **Result:** 200 Top-Tier Tickers selected for training. This ensures the model learns from \"information-rich\" environments.\n",
    "\n",
    "### 2.2 Feature Engineering (The Alpha Factors)\n",
    "We transform raw data into stationary, learnable features:\n",
    "1.  **`Log_Ret`:** Logarithmic returns (Momentum).\n",
    "2.  **`Vol_Ratio`:** Short-term vs. Long-term volatility (Regime Detection).\n",
    "3.  **`Efficiency`:** Fractal Dimension proxy (Trend Quality).\n",
    "4.  **`Vol_Shock`:** Volume spikes relative to 50-day average.\n",
    "5.  **`Sentiment`:** The daily score imported from Notebook 1.1.\n",
    "6.  **`Ret_Lag{X}`:** Deep memory lags (1, 2, 3, 5, 10, 20 days) for the LSTM.\n",
    "\n",
    "### 2.3 Universal Scaling\n",
    "* **Technique:** `RobustScaler` (Scikit-Learn).\n",
    "* **Why:** Financial data has \"Fat Tails\" (Crashes/Explosions). Standard Scaler gets wrecked by outliers. RobustScaler uses the Interquartile Range (IQR), making the model stable across 200 different stocks.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. The Hybrid Architecture\n",
    "\n",
    "We constructed a custom `HybridEnsemble` class that manages two distinct sub-models:\n",
    "\n",
    "### ðŸ§  Model A: The Regime Detector (Random Forest)\n",
    "* **Input:** Volatility, Volume, Efficiency, Sentiment.\n",
    "* **Strength:** Non-linear logic (\"If Volatility is High AND Sentiment is Bad -> SELL\").\n",
    "* **Config:** 100 Trees, Max Depth 5 (to prevent memorization).\n",
    "\n",
    "### ðŸ§  Model B: The Sequence Hunter (LSTM)\n",
    "* **Input:** Time-Series of Returns and Sentiment over 60 days.\n",
    "* **Strength:** Temporal patterns (\"Three days up, one day down...\").\n",
    "* **Config:** 2 Layers, 64 Hidden Units, Dropout 0.2.\n",
    "\n",
    "### âš–ï¸ The Weighted Decision\n",
    "$$P(Buy) = 0.5 \\times P_{RF} + 0.5 \\times P_{LSTM}$$\n",
    "* This equal weighting stabilizes predictions. If one model hallucinates, the other grounds it.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Validation & Stress Testing\n",
    "\n",
    "### 4.1 The Unseen Test (SPY)\n",
    "We trained on 200 stocks but **tested on SPY (S&P 500)**, which the model had *never seen during training*.\n",
    "* **Period:** 2020-2025 (Includes Covid Crash & 2022 Bear Market).\n",
    "* **Result:** The model successfully tracked the S&P 500 trends, avoiding major drawdowns during the 2022 correction.\n",
    "\n",
    "### 4.2 Performance Metrics\n",
    "* **Accuracy:** >55% (Directional).\n",
    "* **Strategy:** Long-Only (Buy when Confidence > 55%).\n",
    "* **Visuals:** The cumulative return plot confirms the AI can identify profitable trends on unseen assets.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Artifact Generation\n",
    "This notebook is the \"Factory\" that produces the final brain files for the live app.\n",
    "\n",
    "1.  **`production_lstm.pth`**: The trained neural network weights.\n",
    "2.  **`production_rf.pkl`**: The trained Random Forest logic.\n",
    "3.  **`scaler_rf.pkl` & `scaler_lstm.pkl`**: The math translators to normalize live data.\n",
    "\n",
    "> **Next Step:** These files are moved to the central `model_artifacts` folder, ready for Phase 3 (The Advisor) and Phase 2 (The Planner)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f293b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96325a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
